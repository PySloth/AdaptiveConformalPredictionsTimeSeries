names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,,] == -Inf] = -100
experts_high[idm,][experts_high[idm,,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(test_size))
experts_high_pred = array(NA, dim=c(test_size))
mlpol_grad_low <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_low[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_high[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
aggregation_gamma_real(dataset, train_size, tab_gamma, alpha, "BOA")
aggregation_gamma_real <- function(dataset, train_size, tab_gamma, alpha, agg){
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
n = dim(data)[1]
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(test_size))
experts_high_pred = array(NA, dim=c(test_size))
mlpol_grad_low <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_low), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_high), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
aggregation_gamma_real(dataset, train_size, tab_gamma, alpha, "BOA")
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
data$target
data.loc[,target]
data[,target]
target
target = 'Spot'
data$target
data[,target]
Y = data[,target]
len(Y)
length(Y)
dim(data)[1]
Y[(train_size+1):length(Y)]
data[,(train_size+1):length(Y)]
data[(train_size+1):length(Y),]
data[(length(Y)-3):length(Y),]
View(data)
rm(list=objects())
library(reticulate)
library(opera)
source("R/loss_conf.R")
source("R/coverage.R")
source_python("R/utils.py")
options("scipen"=1)
alpha = 0.1
tab_gamma = c(0,
0.000005,
0.00005,
0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,
0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,
0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09)
aggregation_gamma_real <- function(dataset, target, train_size, tab_gamma, alpha, agg){
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
Y = data[,target]
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(test_size))
experts_high_pred = array(NA, dim=c(test_size))
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
target = 'Spot'
aggregation_gamma_real(dataset, train_size, tab_gamma, alpha, "BOA")
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "BOA")
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol")
View(data[(train_size+1):,])
dim(data)
View(data)
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
View(data)
dim(data)
View(data[(train_size+1):dim(data)[1],])
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
Y = data[,target]
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(test_size))
experts_high_pred = array(NA, dim=c(test_size))
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
agg = 'BOA'
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
experts_low_pred
View(experts_low_pred)
experts_high_pred
aggregation_gamma_real <- function(dataset, target, train_size, tab_gamma, alpha, agg){
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
Y = data[,target]
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
target = 'Spot'
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "BOA")
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol")
aggregation_gamma_real <- function(dataset, target, train_size, tab_gamma, alpha, agg, gradient){
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
Y = data[,target]
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
if(gradient){names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)}
else{names = get_name_results(paste('Aggregation_',agg,sep=""), dataset=dataset)}
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
target = 'Spot'
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "BOA", gradient = FALSE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol", gradient = False)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol", gradient = FALSE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "EWA", gradient = TRUE)
n_rep = 500
n = 400
train_size = 200
aggregation_gamma <- function(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg){
test_size = n - train_size
name = get_name_data(n, regression='Friedman', noise='ARMA', params_noise=params_noise, seed=n_rep)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/data_cluster"
data = read_pickle(paste(path,"/",name,".pkl",sep=""))
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),n_rep,test_size))
experts_high = array(NA, dim=c(length(tab_gamma),n_rep,test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results_cluster"
names = get_name_results(method, n, regression='Friedman', noise='ARMA',
params_noise=params_noise)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,,] = results$Y_inf
experts_high[idm,,] = results$Y_sup
experts_low[idm,,][experts_low[idm,,] == -Inf] = -1000
experts_high[idm,,][experts_high[idm,,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(n_rep,test_size))
experts_high_pred = array(NA, dim=c(n_rep,test_size))
for(k in 1:n_rep){
mlpol_grad_low <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_low[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_high[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred[k,] = mlpol_grad_low$prediction
experts_high_pred[k,] = mlpol_grad_high$prediction
}
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results_cluster"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), n, regression='Friedman', noise='ARMA',
params_noise=params_noise)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
agg = "EWA"
params_noise = list('ar'=c(1,-0.1), 'ma'=c(1,0.1), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.8), 'ma'=c(1,0.8), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.9), 'ma'=c(1,0.9), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.95), 'ma'=c(1,0.95), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.99), 'ma'=c(1,0.99), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
aggregation_gamma_real <- function(dataset, target, train_size, tab_gamma, alpha, agg, gradient){
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
Y = data[,target]
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
if(gradient){names = get_name_results(paste('Aggregation_Global_',agg,'_Gradient',sep=""), dataset=dataset)}
else{names = get_name_results(paste('Aggregation_Global_',agg,sep=""), dataset=dataset)}
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
dataset = 'Spot_France_ByHour_train_2019-01-01'
train_size = 26136
target = 'Spot'
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "BOA", gradient = TRUE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol", gradient = TRUE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "EWA", gradient = TRUE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "BOA", gradient = FALSE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "MLpol", gradient = FALSE)
aggregation_gamma_real(dataset, target, train_size, tab_gamma, alpha, "EWA", gradient = FALSE)
for(h in 0:23){print(h)}
h = 2
dataset = 'Spot_France_Hour_'+str(h)'_train_2019-01-01'
dataset = 'Spot_France_Hour_'+str(h)+'_train_2019-01-01'
paste('Spot_France_Hour_',str(h),'_train_2019-01-01',sep="")
paste('Spot_France_Hour_',h,'_train_2019-01-01',sep="")
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
target = data$Spot
target
data[,'Spot']
View(data)
data[hour=h,Spot]
data[data$hour==h,Spot]
data[data$hour==h,'Spot']
View(data[data$hour==h,'Spot'])
data[data$hour == h, "Spot"]
View(data["Spot"])
class(data["Spot"])
class(data[data$hour==h,'Spot'])
class(data$Spot)
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
for(h in 0:23){
dataset = paste('Spot_France_Hour_',h,'_train_2019-01-01',sep="")
target = data[data$hour==h,'Spot']
train_size = 1089
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "BOA", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "MLpol", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "EWA", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "BOA", gradient = FALSE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "MLpol", gradient = FALSE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "EWA", gradient = FALSE)
}
aggregation_gamma_real <- function(target, dataset, train_size, tab_gamma, alpha, agg, gradient){
Y = target
n = length(Y)
test_size = n - train_size
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),test_size))
experts_high = array(NA, dim=c(length(tab_gamma),test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
names = get_name_results(method, dataset=dataset)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,] = results$Y_inf
experts_high[idm,] = results$Y_sup
experts_low[idm,][experts_low[idm,] == -Inf] = -100
experts_high[idm,][experts_high[idm,] == Inf] = 1000
}
mlpol_grad_low <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_low), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=Y[(train_size+1):length(Y)], experts=t(experts_high), model = agg, loss.gradient = gradient ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred = mlpol_grad_low$prediction
experts_high_pred = mlpol_grad_high$prediction
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results"
if(gradient){names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), dataset=dataset)}
else{names = get_name_results(paste('Aggregation_',agg,sep=""), dataset=dataset)}
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
data = read.csv("~/Documents/Données/Prices_2016_2019_extract.csv")
for(h in 0:23){
print(h)
dataset = paste('Spot_France_Hour_',h,'_train_2019-01-01',sep="")
target = data[data$hour==h,'Spot']
train_size = 1089
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "BOA", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "MLpol", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "EWA", gradient = TRUE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "BOA", gradient = FALSE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "MLpol", gradient = FALSE)
aggregation_gamma_real(target, dataset, train_size, tab_gamma, alpha, "EWA", gradient = FALSE)
}
plot(qnorm(seq(0,1,length.out=100)))
seq(0,1,length.out=100)
plot(qnorm(seq(0,1,length.out=100)), sd=sqrt(0.1*0.1/(1-0.99*0.99)))
plot(qnorm(seq(0,1,length.out=100), sd=sqrt(0.1*0.1/(1-0.99*0.99))))
?qnorm
sqrt(0.1*0.1/(1-0.99*0.99))
sqrt(0.1*0.1/(1-0.99999*0.99999))
sqrt(0.1*0.1/(1-0.99*0.99))
plot(qnorm(seq(0,1,length.out=100), sd=sqrt(0.1*0.1/(1-0.999*0.999))))
plot(qnorm(seq(0,1,length.out=100), sd=sqrt(0.1*0.1/(1-0.9999*0.9999))))
rm(list=objects())
library(reticulate)
library(opera)
source("R/loss_conf.R")
source("R/coverage.R")
source_python("R/utils.py")
options("scipen"=1)
alpha = 0.1
tab_gamma = c(0,
0.000005,
0.00005,
0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,
0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,
0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09)
### Simulations
n_rep = 500
n = 300
train_size = 200
aggregation_gamma <- function(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg){
test_size = n - train_size
name = get_name_data(n, regression='Friedman', noise='ARMA', params_noise=params_noise, seed=n_rep)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/data_cluster"
data = read_pickle(paste(path,"/",name,".pkl",sep=""))
methods = c()
for(gamma in tab_gamma){
methods = c(methods,paste('ACP_',gamma,sep=""))}
experts_low = array(NA, dim=c(length(tab_gamma),n_rep,test_size))
experts_high = array(NA, dim=c(length(tab_gamma),n_rep,test_size))
for(idm in 1:length(methods)){
method = methods[idm]
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results_cluster"
names = get_name_results(method, n, regression='Friedman', noise='ARMA',
params_noise=params_noise)
results = read_pickle(paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
experts_low[idm,,] = results$Y_inf
experts_high[idm,,] = results$Y_sup
experts_low[idm,,][experts_low[idm,,] == -Inf] = -1000
experts_high[idm,,][experts_high[idm,,] == Inf] = 1000
}
experts_low_pred = array(NA, dim=c(n_rep,test_size))
experts_high_pred = array(NA, dim=c(n_rep,test_size))
for(k in 1:n_rep){
mlpol_grad_low <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_low[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=alpha/2))
mlpol_grad_high <- mixture(Y=data$Y[k,(train_size+1):dim(data$Y)[2]], experts=t(experts_high[,k,]), model = agg, loss.gradient = T ,
loss.type = list(name="pinball", tau=1-alpha/2))
experts_low_pred[k,] = mlpol_grad_low$prediction
experts_high_pred[k,] = mlpol_grad_high$prediction
}
results = list('Y_inf'=experts_low_pred, 'Y_sup'=experts_high_pred)
path = "/Users/mzaffran/Documents/Code/CP/cp-epf/results_cluster"
names = get_name_results(paste('Aggregation_',agg,'_Gradient',sep=""), n, regression='Friedman', noise='ARMA',
params_noise=params_noise)
write_pickle(results, paste(path,"/",names[[1]],"/",names[[2]],".pkl",sep=""))
}
agg = "EWA"
params_noise = list('ar'=c(1,-0.1), 'ma'=c(1,0.1), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.8), 'ma'=c(1,0.8), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.9), 'ma'=c(1,0.9), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.95), 'ma'=c(1,0.95), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.99), 'ma'=c(1,0.99), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
agg = "BOA"
params_noise = list('ar'=c(1,-0.1), 'ma'=c(1,0.1), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.8), 'ma'=c(1,0.8), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.9), 'ma'=c(1,0.9), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.95), 'ma'=c(1,0.95), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.99), 'ma'=c(1,0.99), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
agg = "MLpol"
params_noise = list('ar'=c(1,-0.1), 'ma'=c(1,0.1), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.8), 'ma'=c(1,0.8), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.9), 'ma'=c(1,0.9), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.95), 'ma'=c(1,0.95), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
params_noise = list('ar'=c(1,-0.99), 'ma'=c(1,0.99), 'process_variance'=10)
aggregation_gamma(params_noise, n, train_size, n_rep, tab_gamma, alpha, agg)
?mixture
